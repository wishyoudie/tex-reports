\documentclass[../../calc-math-exam-2023.tex]{subfiles}
\begin{document}
    \section{Метод Гаусса и явление плохой обусловленности. \textbf{LU}-разложение матрицы. Подпрограммы \textbf{DECOMP} и \textbf{SOLVE}.}\label{sec:ch20}

    \subsection{Плохая обусловленность матрицы.}
    Рассмотрим систему нелинейных алгебраических уравнений
    \begin{equation}
        \bf{A}\bf{x} = \bf{b}, \quad \det (\bf{A}) \neq 0 \label{eq:nonlinear_algebraic_equation}
    \end{equation}
    Так как матрица \bf{A} неособеннная, ее единственным решением является вектор
    \begin{equation}
        \bf{x} = \bf{A}^{-1}\bf{b}
    \end{equation}
    Как сильно оно изменится при малой вариации исходных данных (элементов \bf{A} и вектора \bf{b})?

    Численное решение линейных алгебраических систем подвержено влиянию нескольких источников ошибок.
    Два из них традиционны и очевидны: ограниченность разрядной сетки компьютера и погрешность представления
    исходных данных.
    \begin{definition}
        Матрица и вместе с ней система~\eqref{eq:nonlinear_algebraic_equation} называются
        \emph{плохо обусловленными}, если малым изменениям элементов \bf{A} отвечают большие
        изменения элементов $\bf{A}^{-1}$ и, следовательно, сильные изменения вектора решения.
    \end{definition}
    Получим количественную характеристику этого явления. Первоначально будем считать, что
    матрица \bf{A} известна точно, а вектор \bf{b} -- с некотрой погрешностью $\Delta \bf{b}$.
    Тогда система приобретает вид
    \begin{equation*}
        \bf{A}\left( \bf{x} + \Delta \bf{x} \right) = \bf{b} + \Delta \bf{b}
    \end{equation*}
    или после вычитания~\eqref{eq:nonlinear_algebraic_equation} и обращения матрицы:
    \begin{equation*}
        \bf{A}\Delta\bf{x} = \Delta \bf{b} \qquad \Delta \bf{x} = \bf{A}^{-1}\Delta \bf{b}
    \end{equation*}
    Далее при использовании любой ранее рассмотренной нормы матрицы, получаем
    \begin{equation*}
        \left\| \Delta \bf{x} \right\| \leq \left\| \bf{A}^{-1} \right\| \left\| \Delta \bf{b} \right\| \qquad \left\| \bf{b} \right\| \leq \left\| \bf{A} \right\| \left\| \bf{x} \right\|
    \end{equation*}
    Перемножение этих двух неравенств в предположении, что $\bf{b} \neq \bf{O}$, и деление на $\left\| \bf{b} \right\| \cdot \left\| \bf{x} \right\|$ дает
    \begin{equation}
        \frac{\left\| \Delta \bf{x} \right\|}{\left\| \bf{x} \right\|} \leq \textit{cond}(\bf{A})\frac{\left\| \Delta \bf{b} \right\|}{\left\| \bf{b} \right\|},
        \qquad \textit{cond}(\bf{A}) = \left\| \bf{A} \right\| \cdot \left\| \bf{A}^{-1} \right\| \label{eq:matrix_condition}
    \end{equation}
    \begin{definition}
        Число $\displaystyle \textit{cond}(\bf{A}) = \left\| \bf{A} \right\| \cdot \left\| \bf{A}^{-1} \right\|$ называется
        \emph{стандартным числом обусловленности}.
    \end{definition}
    Вычисляя норму от обоих частей равенства $\displaystyle \bf{A} \cdot \bf{A}^{-1} = \bf{E}$, имеем
    $\displaystyle \left\| \bf{A} \right\| \cdot \left\| \bf{A}^{-1} \right\| \geq \left\| \bf{E} \right\|$,
    т.е. $\textit{cond}(\bf{A}) \geq 1$. Равенство~\eqref{eq:matrix_condition} допускает простую интерпретацию
    для практики. Число обусловленности матрицы \bf{A} является верхней границей <<усиления>> относительной
    ошибки вектора \bf{b}, т.е. относительное изменение вектора \bf{b} влечет за собой относительное изменение
    в решении не более чем в $\textit{cond}(\bf{A})$ раз. Если величина $\textit{cond}(\bf{A})$ невелика, то
    говорят о \emph{хорошей} обусловленности матрицы \bf{A}, в противном случае -- о \emph{плохой}.
    \vspace{5pt}

    Теперь рассмотрим ситуацию, когда вектор \bf{b} известен точно, а коэффициенты матрицы \bf{A} заданы с
    погрешностью $\Delta \bf{A}$:
    \begin{gather*}
        \left( \bf{A} + \Delta \bf{A} \right)\left( \bf{x} + \Delta \bf{x} \right) = \bf{b}\\
        \\
        \bf{A}\Delta \bf{x} = - \Delta \bf{A} \left( \bf{x} + \Delta \bf{x} \right)\\
        \left\| \Delta \bf{x} \right\| \leq \left\| \bf{A}^{-1} \right\| \cdot \left\| \Delta \bf{A} \right\| \cdot \left\| \bf{x} + \Delta \bf{x} \right\|
    \end{gather*}
    \begin{equation}
        \frac{\left\| \Delta \bf{x} \right\|}{\left\| \bf{x} + \Delta \bf{x} \right\|} \leq \textit{cond}(\bf{A}) \frac{\left\| \Delta \bf{A} \right\|}{\left\| \bf{A} \right\|}
    \end{equation}
    И в этом случае $\textit{cond}(\bf{A})$ ограничивает сверху увеличение относительной ошибки решения по сравнению с
    относительной ошибкой исходных данных.
    \vspace{10pt}

    Существуют и другие количественные характеристики плохой обусловленности. Например, таким числом является
    величина, отражающая разброс спектра собственных значений \bf{A}:
    \begin{equation*}
        k(\bf{A}) = \frac{\displaystyle |\uplambda_k|_{\textit{max}}}{\displaystyle |\uplambda_k|_{\textit{min}}}
    \end{equation*}
    \emph{Заметим, что} $k(\bf{A}) \leq \textit{cond}(\bf{A})$. Действительно, если $\uplambda_k$ собственные
    значения \bf{A}, то $\displaystyle \frac{1}{\uplambda_k}$ собственные значения $\bf{A}^{-1}$. Поэтому
    $\displaystyle \max |\uplambda_k| \leq \left\| \bf{A} \right\|$, а также $\displaystyle \frac{1}{|\uplambda_k|_{\textit{min}}} \leq \left\| \bf{A}^{-1} \right\|$,
    и для плохо обусловленных матриц имеем:
    \begin{equation*}
        \textit{cond}(\bf{A}) = \left\| \bf{A} \right\| \left\| \bf{A}^{-1} \right\| \geq k(\bf{A}) = \frac{\max|\uplambda_k|}{\min|\uplambda_k|} \gg 1
    \end{equation*}

    \subsection{Метод Гаусса. \textbf{LU}-разложение матрицы.}
    Различают два больших класса методов решения системы~\eqref{eq:nonlinear_algebraic_equation}: \emph{точные} и
    \emph{итерационные}. Точные методы за конечное число арифметических операций при отсутствии ошибок округления
    (что эквивалентно бесконечной разрядной сетке) дают точное решение задачи. В ходе применения итерационных методов
    рождается последовательность векторов, сходящаяся к решению.

    В качестве наиболее популярного представителя методов первой группы рассмотрим метод Гаусса исключения неизвестных.
    Одна из его примитивных модификаций предполагает на первом шаге исключение $\displaystyle x^{(1)}$ с помощью первого
    уравнения из остальных уравнений. С этой целью первое уравнение умножается на $m_{k1} = -a_{k1}/a_{11}$ и
    складывается с $k$-м уравнением и т.д. На втором шаге с помощью преобразованного второго уравнения исключается
    $\displaystyle x^{(2)}$ из последующих уравнений. После исключения $\displaystyle x^{(n-1)}$ завершается так
    называемый \emph{прямой ход} метода Гаусса, результатом которого является треугольная матрица. \emph{Обратный ход}
    метода Гаусса (гораздо менее трудоемкий) сводится к последовательному получению неизвестных, начиная с последнего
    уравнения.

    Алгоритм в таком виде нуждается в существенном замечании. Нельзя заранее предвидеть, что элемент, стоящий в левом
    верхнем углу обрабатываемой матрицы, всегда будет отличен от нуля. Если ситуация с нулевым элементом возникнет, то,
    чтобы избежать деления на нуль, необходимо переставить строки, сделав элемент в этой позиции (ведущий элемент)
    ненулевым. Более того, желательно избегать не только нулевых, но и относительно малых ведущих элементов.

    Наиболее известны следующие две стратегии выбора ведущего элемента:
    \begin{enumerate}
        \item \emph{Полный} выбор.

        Здесь на $k$-м шаге в качестве ведущего берется наибольший по модулю элемент в неприведенной части матрицы.
        Затем строки и столбцы переставляются так, чтобы этот элемент поменялся местами с $a_{kk}$. В этом случае
        каждый раз осуществляется деление на максимальный по модулю элемент, но перестановка столбцов фактически
        сводится к перенумерации компонент вектора $x$.

        \item \emph{Частичный} выбор.

        Здесь на $k$-м шаге в качестве ведущего используют наибольший по модулю элемент первого столбца неприведенной
        части. Затем этот элемент меняют местами с $a_{kk}$, для чего переставляют только строки, избегая перенумерации
        компонент вектора \bf{x}.
    \end{enumerate}

    С современной точки зрения метод Гаусса интерпретируется как разложение матрицы системы~\eqref{eq:nonlinear_algebraic_equation}
    в произведение двух треугольных матрицы (\bf{LU}-разложение). Этот факт отражает следующая теорема, приводимая без
    доказательства.
    \begin{theorem}
        Пусть $\displaystyle \bf{A}^{(k)}$ -- главные миноры квадратной матрицы \bf{A} порядка $m \times m$ ($k=1,2,\dots,m-1$).
        Предположим, что $\det(\bf{A}^{(k)}) \neq 0$. Тогда существует единственная нижняя треугольная матрица
        $\bf{L} = (l_{ij})$, где $l_{11} = l_{22} = \dots = l_{nn} = 1$, и единственная верхняя треугольная матрица
        $\bf{U} = (u_{ij})$, такие, что $\bf{L} \cdot \bf{U} = \bf{A}$. Более того, $\det(\bf{A}) = u_{11} \cdot u_{22} \dots u_{nn}$.
    \end{theorem}

    Эта теорема позволяет представить решение~\eqref{eq:nonlinear_algebraic_equation}
    \begin{equation*}
        \bf{A}\bf{x} = \bf{b} \Longrightarrow \left( \bf{L}\bf{U} \right)\bf{x} = \bf{b} \Longrightarrow \bf{L}\left( \bf{U}\bf{x} \right) = \bf{b}
    \end{equation*}
    как решение двух систем с треугольными матрицами \bf{L} и \bf{U}: $\bf{L}\bf{y} = \bf{b}$ и $\bf{U}\bf{x} = \bf{y}$.
    Решение первой системы с одновременным вычислением \bf{L} и \bf{U} соответсвует прямому ходу метода Гаусса, а решение
    второй системы -- обратному ходу. Технологию \bf{LU}-разложения проиллюстрируем на примере системы четвертого
    порядка без выбора ведущего элемента. Пусть $\displaystyle m_{k1} = -a_{k1}/a_{11}, (k = 2, 3, 4)$. Первый шаг
    прямого хода эквивалентен умножению матрицы \bf{A} и вектора \bf{b} слева на матрицу $\bf{M}_1$:
    \begin{equation*}
        \bf{M}_1 =
        \begin{pmatrix}
            1      & 0 & 0 & 0 \\
            m_{21} & 1 & 0 & 0 \\
            m_{31} & 0 & 1 & 0 \\
            m_{41} & 0 & 0 & 1
        \end{pmatrix},
        \bf{A}_2 = \bf{M}_1 \bf{A}, \, \bf{b}_2 = \bf{M}_1 \bf{b}
    \end{equation*}
    На втором шаге матрица $\bf{A}_2$ и вектор $\bf{b}_2$ умножаются на матрицу $\bf{M}_2$, а на третьем шаге матрица
    $\bf{A}_3 = \bf{M}_2 \bf{A}_2$ и вектор $\bf{b}_3 = \bf{M}_2 \bf{b}_2 $ умножаются на матрицу $\bf{M}_3$.
    $\bf{A}_4 = \bf{M}_3 \cdot \bf{M}_2 \cdot \bf{M}_1 \cdot \bf{A}$
    \begin{equation*}
        \bf{M}_2 =
        \begin{pmatrix}
            1 & 0      & 0 & 0 \\
            0 & 1      & 0 & 0 \\
            0 & m_{32} & 1 & 0 \\
            0 & m_{33} & 0 & 1
        \end{pmatrix},
        \quad \bf{M}_3 =
        \begin{pmatrix}
            1 & 0 & 0      & 0 \\
            0 & 1 & 0      & 0 \\
            0 & 0 & 1      & 0 \\
            0 & 0 & m_{43} & 1
        \end{pmatrix}
    \end{equation*}
    Согласно построению $\bf{A}_4$ есть верхняя треугольная матрица \bf{U}:
    \begin{gather*}
        \bf{M} = \bf{M}_3 \bf{M}_2 \bf{M}_1, \quad \bf{M}\bf{A} = \bf{U}, \quad \bf{L} = \bf{M}^{-1}, \quad \bf{A} = \bf{L}\bf{U},\text{ где}\\
        \bf{L} =
        \begin{pmatrix}
            1       & 0       & 0       & 0 \\
            -m_{21} & 1       & 0       & 0 \\
            -m_{31} & -m_{32} & 1       & 0 \\
            -m_{41} & -m_{42} & -m_{43} & 1
        \end{pmatrix}
    \end{gather*}

    \subsection{Подпрограммы \textbf{DECOMP} и \textbf{SOLVE}.}
    Реализованные в большинстве пакетов по линейной алгебре программы представляют собой набор из двух программ.
    В первой осуществляется \bf{LU}-разложение, а во второй решаются две системы с треугольными матрицами \bf{L}
    и \bf{U} ($\bf{L}\bf{y} = \bf{b}$ и $\bf{U}\bf{x} = \bf{y}$). Примером являются написанные на фортране программы
    \verb|DECOMP| и \verb|SOLVE|. Они имеют следующие параметры
    \begin{verbatim}
DECOMP(NDIM, N, A, COND, IPVT, WORK)
SOLVE(NDIM, N, A, B, IPVT)
    \end{verbatim}
    \bf{NDIM} -- объявленная в описании строчная размерность массива, в котором располагается матрица \bf{A}

    \bf{N} -- порядок системы уравнений

    \bf{A} -- матрица, подвергающаяся разложению (по окончании работы программы на ее месте располагаются матрицы \bf{L} и \bf{U})

    \bf{COND} -- оценка числа обусловленности

    \bf{IPVT} -- вектор индексов ведущих элементов (размерность \bf{N})

    \bf{WORK} -- рабочий одномерный массив (размерность \bf{N})

    \bf{B} -- вектор правых частей системы~\eqref{eq:nonlinear_algebraic_equation}, где по окончании работы программы
    \verb|SOLVE| размещается вектор решения \bf{x}.

    В заключение оценим число арифметических операций в методе Гаусса. На каждом шаге исключения мы встречаемся с
    операциями деления и умножения-вычитания. Возьмем за единицу измерения операцию именно такого типа. На $k$-м шаге
    в одной строке выполняется одно деление и $k$ умножений-вычитаний. Тогда для всех $k - 1$ строк имеем:
    $(k+1)(k-1) = k^2 - 1$ операций. В прямом ходе Гаусса таких шагов $m$. В итоге получаем:
    \begin{equation*}
        \sum_{k=1}^{m} \left( k^2 - 1 \right) = \sum_{k=1}^{m} k^2 - m = \frac{2m^3 + 3m^2 - 5m}{6}
    \end{equation*}
    При больших значениях $m$ хорошим приближением для числа операций будет $m^3/3$. Для обратного хода нужно на
    порядок меньше операций (одно деление и $k - 1$ умножение-вычитание при вычислении $\displaystyle x^{(k)}$, что
    для всех компонент дает величину $\displaystyle \sum_{k=1}^{m} \frac{m^2 + m}{2}$).
\end{document}