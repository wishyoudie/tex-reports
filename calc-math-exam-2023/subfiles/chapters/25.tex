\section{Методы Адамса. Локальная и глобальная погрешности, степень метода.}\label{sec:ch25}

\subsection{Методы Адамса.}
Рассмотрим другой подход решения задачи Коши~\eqref{eq:koshi_1}. Для построения решения в точке $t_{n+1}$ будем
использовать информацию в ранее полученных точках $t_n, t_{n-1}, \ldots$. Так, по двум предыдущим точкам $t_n$ и
$t_{n-1}$ построим интерполяционный полином первой степени для функции $\bf{f}(t, \bf{x})$
\begin{equation*}
    \bf{f}\left( \tau, \bf{x}(\tau) \right) \approx \frac{\tau - t_{n-1}}{t_n - t_{n-1}}\bf{f}_n + \frac{\tau - t_n}{t_{n-1} - t_n}\bf{f}_{n-1}
\end{equation*}
и подставим его в формулу~\eqref{eq:koshi_2}. Попутно \emph{заметим, что} полином используется вне промежутка
интерполирования, т.е. проводится \emph{экстраполяция}. Получаем следующий численный метод:
\begin{equation}
    \bf{x}_{n+1} = \bf{x} + \frac{h}{2}\left( 3\bf{f}_n - \bf{f}_{n-1} \right) \label{eq:adams_for_2}
\end{equation}
Использования трех точек $t_n, t_{n-1}, t_{n-2}$ и полинома второй степени приведет к формуле
\begin{equation}
    \bf{x}_{n+1} = \bf{x}_n + \frac{h}{12}\left( 23\bf{f}_n - 16\bf{f}_{n-1} + 5\bf{f}_{n-2} \right) \label{eq:adams_for_3}
\end{equation}
а для четырех точек разностная схема алгоритма принимает вид
\begin{equation}
    \bf{x}_{n+1} = \bf{x}_n + \frac{h}{24}\left( 55\bf{f}_n - 59\bf{f}_{n-1} + 37\bf{f}_{n-2} - 9\bf{f}_{n-3} \right) \label{eq:adams_for_4}
\end{equation}

Все эти методы получили название \emph{методов Адамса}. Они принадлежат семейству многошаговых алгоритмов, разностные
уравнения которых имеют порядок выше первого. Методы~\eqref{eq:adams_for_2} --~\eqref{eq:adams_for_4} являются
\emph{явными} методами Адамса. Если в состав точек, по которым строится полином, включить $t_{n+1}$, то возникают
\emph{неявные} методы Адамса. Для двух точек $t_{n+1}, t_n$ получается метод трапеций~\eqref{eq:diff_traps}, а для
трех точек  $t_{n+1}, t_n, t_{n-1}$ и четырех точек $t_{n+1}, t_n, t_{n-1}, t_{n-2}$ -- следующие два метода
\begin{gather}
    \bf{x}_{n+1} = \bf{x}_n + \frac{h}{12}\left( 5\bf{f}_{n+1} + 8\bf{f}_n - \bf{f}_{n-1} \right) \label{eq:adams_for_2p1} \\
    \bf{x}_{n+1} = \bf{x}_n + \frac{h}{24}\left( 9\bf{f}_{n+1} + 19\bf{f}_n - 5\bf{f}_{n-1} + \bf{f}_{n-2} \right) \label{eq:adams_for_3p1}
\end{gather}
\vspace{5pt}

Несомненным достоинством явных методов Адамса является тот факт, что все они независимо от своей точности требуют
лишь однократного вычисления функции $\bf{f}(t, \bf{x})$ на одном шаге и конкурировать с ними в этом плане весьма
трудно. Остальные значения производной решения берутся с предыдущих шагов. Вместе с тем, методы Адамса, как и другие
многошаговые алгоритмы, не являются самостартующими, т.е. они требуют для начала интегрирования специальных стартовых
алгоритмов для расчета дополнительных начальных условий. В качестве этих стартовых методов могут быть использованы
любые другие методы, например методы Рунге-Кутты, или специально разработанные для этих целей алгоритмы.

Неявные методы Адамса могут использоваться как сами по себе (тогда на каждом шаге решаются нелинейные уравнения
относительно $\bf{x}_{n+1}$), так и в паре с явными методами. В последнем случае значение $\bf{x}_{n+1}$ сначала
оценивается явным методом ($\displaystyle \bf{x}_{n+1}^{\text{Э}}$), а затем уточняется неявным алгоритмом.

Например, такую пару методов образуют методы~\eqref{eq:adams_for_4} и~\eqref{eq:adams_for_3p1}:
\begin{gather}
    \displaystyle \bf{x}_{n+1}^{\text{Э}} = \bf{x}_n + \frac{h}{24}\left( 55\bf{f}_n - 59\bf{f}_{n-1} +37\bf{f}_{n-2} -9\bf{f}_{n-3}\right) \label{eq:extra_1} \\
    \displaystyle \bf{x}_{n+1}^{\text{Э}} = \bf{x}_n + \frac{h}{24}\left( 9\bf{f}(t_{n+1}, \bf{x}_{n+1}^{\text{Э}}) + 19\bf{f}_n - 5\bf{f}_{n-1}  + \bf{f}_{n-2}\right) \label{eq:extra_2}
\end{gather}
В зарубежной литературе совместное использование явного и неявного методов называют \emph{методами прогноза-коррекции}.
В нашей литературе часто используют термин \emph{экстраполяционные} методы для~\eqref{eq:extra_1} и
\emph{интерполяционные} методы для~\eqref{eq:extra_2}.

\subsection{Локальная и глобальная погрешности, степень метода.}
Теперь обратимся к анализу погрешности численных методов и начнем с самого простого алгоритма -- явного метода
ломаных Эйлера. Рассмотрим частный случай формулы~\eqref{eq:diff_left_triangles}, когда функция $\bf{f}(t, \bf{x})$
не зависит от \bf{x}. Тогда явный метод ломаных Эйлера превращается в квадратурную формулу левых прямоугольников:
\begin{equation}
    \bf{x}_{n+1} = \bf{x}_n + h\bf{f}\left( t_n \right) = \bf{x}_0 + h \sum_{k=0}^{n} \bf{f}_k \label{eq:diff_left_triangles_2}
\end{equation}
При этом общая погрешность в точке $t_n$ является точной суммой погрешностей, допущенных на каждом отдельном шаге.

Иная ситуация складывается, когда $\bf{f}(t, \bf{x})$ зависит от \bf{x}. Только погрешность первого шага формулы~\eqref{eq:diff_left_triangles}
при $n = 0$ вычисляется аналогично~\eqref{eq:diff_left_triangles_2}.
\begin{equation*}
    \bf{x}_1 = \bf{x}_0 + h\bf{f}(t_0, \bf{x}_0)
\end{equation*}
Уже на втором шаге при $n = 1$ эта погрешность сложным образом зависит от погрешности первого шага, так как при
вычислении $\bf{f}(t_1, \bf{x}_1)$ используется приближенное значение $\bf{x}_1$
\begin{equation*}
    \bf{x}_2 = \bf{x}_1 + h\bf{f}(t_1, \bf{x}_1)
\end{equation*}
В общем случае на $n$-м шаге погрешность очень сложно зависит от всех погрешностей, допущенных на предыдущих шагах.
Разностное уравнение метода может оказаться неустойчивым, и тогда происходит неприемлемый рост погрешности.

Устойчивость разностной схемы связана с выбранным методом, шагом интегрирования и видом функции $\bf{f}(t, \bf{x})$.
Важно так выбрать сам метод и шаг для него, чтобы погрешность решения была бы приемлемой. В соответствии со сказанным
вводятся погрешности двух видов.
\begin{definition}[Локальная погрешность]
    Это погрешность, допущенная на одном шаге при условии, что решение во всех предыдущих точках вычислено точно.
\end{definition}
\begin{definition}[Глобальная погрешность]
    Это разность между точным и приближенным решением на $n$-м шаге.
\end{definition}
Именно глобальная погрешность является истинной погрешностью. Локальная погрешность совпадает с ней лишь на первом
шаге. Однако, в общем случае оценка глобальной погрешности крайне затруднена, а чаще невозможна, и поэтому оценивают
локальную погрешность на каждом шаге.

Малая величина локальной погрешности вовсе не гарантирует малую величину глобальной, но если есть уверенность, что
устойчивость разностного уравнения метода обеспечена, то из малой величины локальной погрешности следует, что
глобальная не будет слишком велика. Будем пока полагать, что устойчивость обеспечена, и рассмотрим подробнее
характеристики локальной погрешности.

Важной характеристикой является \emph{степень} (или \emph{порядок точности}) метода. Все ранее рассмотренные методы
могут быть записаны в следующем виде:
\begin{equation}
    \bf{x}_{n+1} = \bf{x}_n + h\bf{F}\left( t_n, h, \bf{x}_{n+1}, \bf{x}_n, \bf{x}_{n-1}, \ldots, \bf{x}_{n - k} \right) \label{eq:diff_common_method_form}
\end{equation}
Разложим выражение в правой части равенства в ряд Тейлора по степеням $h$ в точке $t_n$:
\begin{equation}
    \bf{x}_{n+1} = \bf{x}\left( t_n + h \right) = \bf{x}_n + \sum_{k=1}^{\infty} \alpha_k(t_n) h^k \frac{d^k \bf{x}(t_n)}{dt^k} \label{eq:dcmf_taylor}
\end{equation}
где коэффициенты $\alpha_k(t_n)$ определяются выбранным методом.

С другой стороны, значение $\bf{x}_{n+1} = \bf{x}(t_n + h)$ может быть представлено, в свою очередь, точным разложением
в ряд
\begin{equation}
    \bf{x}_{n+1} = \bf{x}(t_n + h) = \bf{x}_n + \sum_{k=1}^{\infty} \frac{h^k}{k!} \frac{d^k\bf{x}(t_n)}{dt^k} \label{eq:dcmf_precise}
\end{equation}

Метод имеет степень $s$, если коэффициенты разложения~\eqref{eq:dcmf_taylor} совпадают с соответствующими
коэффициентами~\eqref{eq:dcmf_precise} до $h^s$ включительно. В качестве примера определим степень некоторых ранее
полученных методов.

Для явного метода ломаных Эйлера:
\begin{equation*}
    \bf{x}_{n+1} = \bf{x}_n + h\bf{f}(t_n, \bf{x}_n) = \bf{x}_n + h\bf{x}'(t_n)
\end{equation*}
Вычитая эту формулу из~\eqref{eq:dcmf_precise}, видим, что коэффициенты совпадают лишь при $h^1$, и для локальной
погрешности этого метода первой степени справедлива оценка: $\displaystyle \bm{\varepsilon}_{n+1} = \frac{h^2 \bf{x}''(\eta)}{2}$.
Первую степень имеет и неявный метод ломаных Эйлера~\eqref{eq:diff_right_triangles} с локальной погрешностью
$\displaystyle \bm{\varepsilon}_{n+1} = - \frac{h^2 \bf{x}''(\eta)}{2}$.

Аналогично убеждаемся, что метод Адамса имеет вторую степень (совпадают члены разложения при $h^1$ и $h^2$)
\begin{flalign*}
    &\displaystyle \bf{x}_{n+1} = \bf{x}_n + \frac{h}{2}\left( 3\bf{x}'(t_n) - \bf{x}'(t_n - h) \right) = \bf{x}_n + \frac{h}{2}\left( 3\bf{x}'(t_n) - \bf{x}'(t_n) + h\bf{x}''(t_n) - \frac{h^2}{2}\bf{x}'''(t_n) + \ldots \right) = \\
    &\displaystyle = \bf{x}_n + h\bf{x}'(t_n) + \frac{h^2}{2}\bf{x}''(t_n) - \frac{h^3}{4}\bf{x}'''(t_n) + \ldots
\end{flalign*}
а после вычитания этого выражения из~\eqref{eq:dcmf_precise} получаем оценку локальной погрешности:
$\displaystyle \bm{\varepsilon}_{n+1} = \frac{5h^3\bf{x}'''(\eta)}{12}$. Также второй порядок точности будет и у
метода трапеций~\eqref{eq:diff_traps}
\begin{flalign*}
    &\displaystyle \bf{x}_{n+1} = \bf{x}_n + \frac{h}{2}\left( \bf{f}(t_n, \bf{x}_n) + \bf{f}(t_{n+1}, \bf{x}_{n+1}) \right) = \bf{x}_n + \frac{h}{2}\left( \bf{x}'(t_n) + \bf{x}'(t_n + h) \right) = \\
    &\displaystyle = \bf{x}_n + \frac{h}{2}\left( \bf{x}'(t_n) + \bf{x}'(t_n) + h\bf{x}''(t_n) + \frac{h^2}{2}\bf{x}'''(t_n) + \ldots \right) = \\
    &\displaystyle = \bf{x}_n + h\bf{x}'(t_n) + \frac{h^2}{2}\bf{x}''(t_n) + \frac{h^3}{4}\bf{x}'''(t_n) + \ldots
\end{flalign*}

Можно убедиться, что методы Адамса имеют третью степень, а методы~\eqref{eq:adams_for_4} и~\eqref{eq:adams_for_3p1}
-- четвертую степень соответственно. Главный член погрешности метода $s$-й степени содержит, как множитель, величину $\displaystyle h^{s+1}$
